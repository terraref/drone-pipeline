# READ THIS
# The Travis CI environment needs to have the following defined:
# 1. DOCKER_COMPOSE_VERSION - the version of docker-compose to install
# Additional information can be found at https://docs.google.com/document/d/1XluZEZNzYHW6rDXrJOrddhE0lG9lTj1-JvRoKjiN6Kg/edit?usp=sharing

# Superuser for docker commands and such
sudo: required

# python environment and python version
language: python
python: 
  - "3.6"
cache: pip

# What sources to auto build
branches:
  only:
  - master
  - develop
  - "/.*travis.*/" # indicates a Travis Ci test
  # - "/^\\d+\\.\\d+(\\.\\d+)?(-\\S*)?$/" # tagged branches

# We're using docker
services:
  - docker

# Only deploy the container after the master branch has built
stages:
  - name: after_script
    # require the branch name to be master (note for PRs this is the base branch name)
    if: branch = master

# List all extractors here. The individual scripts will check the branch against their job
matrix:
  include:
    - name: "OpenDroneMap Extractor"
      env: EXTRACTOR_BRANCH=opendronemap EXTRACTOR_RUN_EXTRA_PARAMS= TEST_SOURCE_ARCHIVE=gdrive:1E-lf6ADbkLONHyvZjiQRZ9CR2WmQ4nua/odm_test_data.tar TEST_COMPARE_ARCHIVE=gdrive:1nF2AhO9UCoRaV0Tul6d2n7LjRZs6nT1Y/odm_results.tar EXTRACTOR_MASTER_DOCKERHUB_NAME=chrisatua/extractors:terra-odm RESULT_DATASET_NAME= RESULT_DATASET_FILE_MATCH=rgb_.tif,.laz RESULT_DATASET_FILTER= VALIDATION_PARAMS="-pixdiff=2 -geotiffclip=408978.24,3660495.25,408993.26,3660488.58 -bychannel=0.5,4.0" SILENCE_FOLDER_LISTING=
      if: branch = master OR branch = develop OR branch =~ /.*opendronemap.*/ OR branch =~ /.*odm.*/ OR branch =~ /.*travis.*/
#    - name: "Clip By Shape Extractor"
 #     env: EXTRACTOR_BRANCH=clipbyshape EXTRACTOR_RUN_EXTRA_PARAMS="-e 'MOUNTED_PATHS={\"/home/extractor/sites\":\"/home/extractor/sites\"}'" TEST_SOURCE_ARCHIVE=gdrive:1_kYL1gSb1wmiokIYN3dgU8baVPS01R9K/clipbyshape_test_data.tar TEST_COMPARE_ARCHIVE=gdrive:1eCQhlNtMlKqB73REmbrSAWE4pDDXO9IM/clipbyshape_results.tar EXTRACTOR_MASTER_DOCKERHUB_NAME=chrisatua/extractors:clipbyshape RESULT_DATASET_NAME= RESULT_DATASET_FILE_MATCH=.tif RESULT_DATASET_FILTER="- (.+?) -" VALIDATION_PARAMS= SILENCE_FOLDER_LISTING=true
  #    if: branch = master OR branch = develop OR branch =~ /.*clipbyshape.*/ OR branch =~ /.*travis.*/
    - name: "Canopy Cover Value Extractor"
      env: EXTRACTOR_BRANCH=canopycover EXTRACTOR_RUN_EXTRA_PARAMS= TEST_SOURCE_ARCHIVE=gdrive:1xqwD3KGNjSXq01tTJgeM-JHQiJbUH4pg/canopycover_test_data.tar TEST_COMPARE_ARCHIVE=gdrive:1uw4judndF8b50oSC8_Qhk5znH18iK-zR/canopycover_results.tar EXTRACTOR_MASTER_DOCKERHUB_NAME=chrisatua/extractors:canopycover RESULT_DATASET_NAME= RESULT_DATASET_FILE_MATCH=.csv RESULT_DATASET_FILTER= VALIDATION_PARAMS= SILENCE_FOLDER_LISTING=
      if: branch = master OR branch = develop OR branch =~ /.*canopycover.*/ OR branch =~ /.*travis.*/

# Secure stuff: travis encrypt --com '<var>=<value>'
#     DockerHub credentials
env:
  global:
  - secure: "pQxyEFC1GGc31omcksaro1LItUT6ou3vjXtsRTBfVNzrjKwwUBoaGYpuOn3z/H12aAlI1tlPpi/WkMde1aZj81b4uwDjhfxd3LrAsIMvnsiMBinng10GPoJV9EZIIlh0YF+QjHK923bLSu24CepcAzEhI49U7ReLYLKB7kljflc6vO/XHqslAUeCTogLGL9terUjJvW9mJ5KRZltth1orC84ehFcGQ+kE5HlLNAdiBRooGprF95g2rg5PiJTp5zQCLIQigVKrJPsHJeS8CQmC2Z6kRdJSooy0NHdeIFn+QKTVr0ZJ9Y0p81QQxG/X591tEGKZr8HsiBaaNouP2V8D8UbwMwm29qS/QIYHD5/EwcCCVExmKDR3nNOTNHts2QPVxsNqVWLGxZDFT5EuHEFg4+TBKADSoK1Lo5r80FQiZtq0zSZU3IkPez1VxwOdNQ2PrA1AGAQPa8uGBvtHPKCKa4exiBAU+D+G7JWc1evTkOIRJnla38Rx8tZUD6ZFskokeaXECNGqJji0ZaPdIpUevXv/i18W+kUPxfMMADzl/C+8U6CYxEAqoKfcUOx3xSckUL6ADwx4BYL8ubFn7wiMQlURtpbuCD8NUOLF55EIx8miCtCWxHUC2t9PN888sFOBnsH1V7yTykXcJ+qZQd5b9OMLVkoXOuCcI+HAqEbd1s="
  - secure: "FXJHqo1Nym1Sgxxf6ulBqlDz75doEvDKtVPgkqHyMV+2yI/AAxK7dvXQLMvkuZq+2IEQwj47bjPrBhSYuH7AA9wPptRWY20G6wurJFPwhom641ZtqitPIPHxpWRIU7RDnHlnYzp4yfZ3GICJrTkjr1JibT84sIoIg2cQF/K68k2XQYefAJQeoerlWi0mMCtJ1oLy1q3ij7DdRDZfX0Qs2Nh3evbLlhyn6quMjc8sRSfS5iVUwNrEfc3NQM9lEzLLYxTcIfwdTDui9+x8ktj8WdSDpUgmThGsOnTIYj/tsgUApLGZIC/Rk12E3df0VYnbaxN63hSwMFdDbKWlll9ZoTlFFKTGlz96DeOtfTPxzhN7XcAA2/hN9dnkyaRyfoxS7xSlTj8iJBoMfKrf1nQVGsELSIYXpI2EqVNkCYfOxJ0Y/bEqYVya+RNa/9a3KNSOTyeGLHtd7YBvtWchJ38OyXA9++DVYQHJCnHatajLzB3EZlbkjDIx7e5mFy5y9pCqGYLzFdPBNgPFN/pO2EbjPWrfnWbHWW3YeVm03odhpq/FP5ECoc4la2fEkQ3ZSEz3YopitdeXMZtFVk1H+ftFRu5vHspRq2sNnKaNPzZTrI7yRmBbt4hsTXQBn7v9e4R+rmLPZjGs24rPeTNkoUZy8l/X9QQ+zkcZibwLjMh5Iqc="
  - CLOWDER_HOST_URI=http://localhost:9000
  - DOCKER_NAMED_CONTAINER=test_extractor   # Used to name the container for easier finding
  - TEST_CLOWDER_USERNAME=test@example.com
  - TEST_CLOWDER_PASSWORD=testPassword
  - TEST_DATASET_NAME=test_dataset
  - TEST_SPACE_NAME=test_space
  - MONGO_INIT_CONTAINER=clowder/mongo-init:latest

# Basic setup for the build. Install software needed by all extractors followed by optional installs for specific extractors
before_install:
  - echo "Working with job:\ $EXTRACTOR_BRANCH"
  - echo "Checking branch name against folder structure"
  - export BRANCH=$(if [ "$TRAVIS_PULL_REQUEST" == "false" ]; then echo $TRAVIS_BRANCH; else echo $TRAVIS_PULL_REQUEST_BRANCH; fi)
  - echo "Travis branch name is:\ $BRANCH"
  - BUILD_FOLDER=`./test/check_branch_acceptable.sh "$BRANCH" "$EXTRACTOR_BRANCH"`
  - echo "Verified build folder is ${BUILD_FOLDER} for branch ${BRANCH}"
  - echo "Displaying basic environment information"
  - docker --version
  - ls -l
  - ls -l test
  - echo "Attempting to update to docker-compose version:\ $DOCKER_COMPOSE_VERSION"
  - sudo rm -f /usr/local/bin/docker-compose   # Update docker-compose by removing any old versions
  - sudo curl -L "https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
  - sudo chmod +x /usr/local/bin/docker-compose
  - ls -l /usr/local/bin/docker-compose     # Show that docker-compose is in place with correct permissions
  - docker-compose --version
  # Install common packages
  - echo "Installing packages"
  - pip install -U pyclowder
  # Extractor dependency installations
  - sudo apt install -y python3-openssl
  - sudo apt install -y libjpeg-dev
  - sudo apt install -y libtiff-dev
  - sudo apt-get install -y gdal-bin
  - sudo apt-get install -y python-opencv
  - sudo apt-get install -y python3-numpy
  - python --version
  - python -m pip install -U 'requests[secure]'
  - python -m pip install -U 'urllib3[secure]'
  - python -m pip install -U 'opencv-python-headless[secure]'

# Start up the test bed containers and copy files from other locations as needed
install:
  - echo "Starting up environment with docker-compose"
  - docker-compose -p clowder -f test/docker-compose.yml up -d
  - docker ps -a
  - docker inspect clowder_clowder_1
  - ./test/wait_for_clowder.sh
  # Setup the clowder account
  - echo "Starting Clowder configuration"
  - echo "Clowder URL:\ $CLOWDER_HOST_URI"
  - docker run -it --rm --network clowder_clowder -e "MONGO_URI=mongodb://mongo:27017/clowder" -e "FIRST_NAME=test" -e "LAST_NAME=test" -e "EMAIL_ADDRESS=$TEST_CLOWDER_USERNAME" -e "USERNAME=$TEST_CLOWDER_USERNAME" -e "PASSWORD=$TEST_CLOWDER_PASSWORD" -e "ADMIN=false" $MONGO_INIT_CONTAINER
  - docker rmi $MONGO_INIT_CONTAINER  # Clean up the container to save space that night be needed
  - sleep 5
  - export API_KEY=`curl -X POST -v -H "Content-Type:application/x-www-form-urlencoded" "$CLOWDER_HOST_URI/api/users/keys?name=testingkey" -u "$TEST_CLOWDER_USERNAME:$TEST_CLOWDER_PASSWORD" | sed -r 's/.*"key":"(.+)".*/\1/'`
  - echo "API KEY:\ $API_KEY"

# Setup the clowder environment
before_script:
  - echo "Setting up Clowder spaces and datasets"
  - export SPACE_ID=`curl -X POST -v -H "accept:application/json" -H "Content-Type:application/json" -d "{\"name\":\"$TEST_SPACE_NAME\",\"description\":\"Test results\"}" "$CLOWDER_HOST_URI/api/spaces?key=$API_KEY" | sed -r 's/.*id\"\:\"(.+)\".*/\1/'`
  - echo "SPACE_ID:\ $SPACE_ID"
  - export DATASET_ID=`curl -X POST -v -H "accept:application/json" -H "Content-Type:application/json" -d "{\"name\":\"$TEST_DATASET_NAME\"}" "$CLOWDER_HOST_URI/api/datasets/createempty?key=$API_KEY" | sed -r 's/.*id\"\:\"(.+)\".*/\1/'`
  - echo "DATASET ID:\ $DATASET_ID"
  - echo "Building and running the extractor to test using Dockerfile in folder $BUILD_FOLDER"
  - export CONTAINER_NAME="chrisatua/test:$EXTRACTOR_BRANCH"
  - echo "Tagging container as:\ $CONTAINER_NAME"
  - docker build -t $CONTAINER_NAME "./$BUILD_FOLDER"
  - docker run --network clowder_clowder -e 'RABBITMQ_URI=amqp://rabbitmq/%2F' -e 'RABBITMQ_EXCHANGE=terra' -d "--name=$DOCKER_NAMED_CONTAINER" $EXTRACTOR_RUN_EXTRA_PARAMS "$CONTAINER_NAME"
  - export EXTRACTOR_CLOWDER_NAME=`cat $BUILD_FOLDER/extractor_info.json | grep '\"name\"' | sed -r 's/\"name\"\:.*\"(.+)\",/\1/' | tr -d '[:space:]'`
  - echo "Found extractor name of:\ $EXTRACTOR_CLOWDER_NAME"
  - echo "Loading the data for the extractor test"
  - ./test/extract.sh "$TEST_SOURCE_ARCHIVE" "data" # Uncompress the test files into the data folder
  - ls -l ./data
  - ./test/update_yaml.sh "./data/experiment.yaml" "$SPACE_ID" "$TEST_CLOWDER_USERNAME" "$TEST_CLOWDER_PASSWORD"
  - cat ./data/experiment.yaml
  - ./test/upload_data.py                     # Put the files into clowder
  - echo "Registering the extractor and waiting for it to start"
  - ./test/register_extractor.py $BUILD_FOLDER/extractor_info.json
  - ./test/wait_for_started.py "$CONTAINER_NAME"

# Start the extraction process and get a result
script:
  - echo "Starting the extractors work"
  - docker exec -t clowder_rabbitmq_1 /opt/rabbitmq/escript/rabbitmq-diagnostics list_bindings
  - docker exec -t clowder_rabbitmq_1 /opt/rabbitmq/escript/rabbitmq-diagnostics list_queues
  - ./test/start_extract.py "$EXTRACTOR_CLOWDER_NAME" "$CLOWDER_HOST_URI/api/datasets/$DATASET_ID/extractions?key=$API_KEY"
  - docker exec -t clowder_rabbitmq_1 /opt/rabbitmq/escript/rabbitmq-diagnostics list_queues
  - echo "Waiting for extractor to finish and setting up for test result validation"
  - travis_wait 30 ./test/wait_for_finish.py "$CONTAINER_NAME"
  - ./test/extract_results.py  "$RESULT_DATASET_NAME"      # get the results
  - ./test/extract.sh "$TEST_COMPARE_ARCHIVE" compare  # setup any comparables
  - echo "Running the validation of extractor produced data"
  - /bin/bash -c '[[ -z "$SILENCE_FOLDER_LISTING" ]] && ls -la || echo "Skipping folder listing"'
  - /bin/bash -c '[[ -z "$SILENCE_FOLDER_LISTING" ]] && ls -la ./datasets || echo "Skipping folder listing"'
  - /bin/bash -c '[[ -z "$SILENCE_FOLDER_LISTING" ]] && [[ ! -z "$RESULT_DATASET_NAME" ]] && ls -la "./datasets/$RESULT_DATASET_NAME" || echo "Skipping folder listing"'
  - /bin/bash -c '[[ -z "$SILENCE_FOLDER_LISTING" ]] && ls -la ./compare || echo "Skipping folder listing"'
  - ./test/validate_results.py  "$RESULT_DATASET_FILE_MATCH" "$RESULT_DATASET_FILTER" $VALIDATION_PARAMS   # verify the results

after_script:
  - echo "Deploying container after successful run"
  - docker tag "$CONTAINER_NAME" "$EXTRACTOR_MASTER_DOCKERHUB_NAME"
  - echo "$DOCKER_PASSWORD" | docker login --username "$DOCKER_USERNAME" --password-stdin
  - docker push "$EXTRACTOR_MASTER_DOCKERHUB_NAME"
